title: Introduce CI/CD pipeline for epicstory
createdAt: 2024-04-18 17:24
createdBy: omateusmento@gmail.com
startedAt: 2024-04-20 13:14
description: >
  Although the deployment strategy is being able to build the infrastructure for deploying the epicstory-api
  by leverage the automation of terraform, it is still manual since I have to execute a terraform command.
  Also to build the application a docker image is being built and store inside the deployment ec2 instance
  instead of in separate locations where it can be built and store independently.
  
  I want to introduce a CI/CD pipeline where a docker image is built for the application, stored in a image
  registry and deployed into a ec2 instance where the image is run.

  A few options I already have in mind. For the image registry, Dockerhub and AWS ECR are two good options.
  For the CI/CD pipeline, I could use either Github Actions or CodePipeline. I see that AWS have other related
  services such as CodeBuild, CodeCommit and CodeDeploy as well.
comments:
  - createdAt: 2024-04-19 11:57
    createdBy: omateusmento@gmail.com
    content: >
      I've being learning AWS ECR until this moment and so far I was able to publish a docker image in a repository
      at AWS ECR. It only needed a few AWS CLI commands:
      ```
      # Building docker image
      docker build -t epicstory-api:0.1.0 .

      # Creating AWS ECR repository
      aws ecr create-repository --repository-name epicstory-api --region sa-east-1

      # Login docker to AWS ECR (needed for uploading and downloading the image)
      aws ecr get-login-password --region sa-east-1 | docker login --username AWS --password-stdin 429249706241.dkr.ecr.sa-east-1.amazonaws.com

      # Publishing docker image
      docker tag epicstory-api:0.1.0 429249706241.dkr.ecr.sa-east-1.amazonaws.com/epicstory-api:0.1.0
      docker push 429249706241.dkr.ecr.sa-east-1.amazonaws.com/epicstory-api:0.1.0

      # Publishing docker image
      docker run -it -d -p 80:3000 429249706241.dkr.ecr.sa-east-1.amazonaws.com//epicstory-api:0.1.0
      ```
  - createdAt: 2024-04-19 12:05
    createdBy: omateusmento@gmail.com
    content: >
      I've been learning Github Actions and figuring out how to achieve the same results. So far I saw that I could
      use an action to apply those commands but I would need a step for configuring the aws cli with the action
      aws-actions/configure-aws-credentials and login docker to AWS ECR with aws-actions/amazon-ecr-login
  - createdAt: 2024-04-21 20:24
    createdBy: omateusmento@gmail.com
    content: |
      I was reading through some github actions documentations and figure that I could run different workflows
      for different set of configuration. I also was able to run steps with shell scripts in the selected OS. In order
      to run tests, for example, I could simply include a step to run the test commands in the workflow (i.e. `npm test`).
      Github actions also include docker and git, so I could use testcontainers in my tests without docker setup and
      push changes to my repository in a workflow.
  - createdAt: 2024-04-21 20:52
    createdBy: omateusmento@gmail.com
    content: |
      Github Actions seems to work seemsly with integration tests with docker and testcontainers. I was able to create
      a workflow for running tests with testcontainers without any additional setup. I only executed a test command and
      testcontainers was able to create the postgresql container needed for the test. The tests results are displayed in
      the actions execution page.
  - createdAt: 2024-04-22 08:42
    createdBy: omateusmento@gmail.com
    content: |
      I think that for this issue a CI/CD pipeline that can test the application, build and publish a docker image and
      deploy a containerized application in aws ec2 instance using terraform would be enough for now.
  - createdAt: 2024-04-22 11:47
    createdBy: omateusmento@gmail.com
    content: |
      Great, I was able to introduce a github action that builds and publishes a docker image into AWS ECR. I needed
      to configure aws with access keys in the workflow as well as providing repository secrets for those keys. I also
      needed to login docker to the AWS ECR registry to be able to publish the docker image later in another step.

      All was need is two actions (aws-actions/configure-aws-credentials and aws-actions/amazon-ecr-login) to configure
      aws and login docker to aws ecr, plus some shell commands for building and publishing of the docker image:

      ```yaml
      - name: Configure AWS CLI
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Login Docker to AWS ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
        with:
          registries: ${{ secrets.AWS_REGISTRY }}
      - name: Build and publish docker image
        run: |
          cd api
          docker build -t epicstory-api .
          docker tag epicstory-api ${{ steps.login-ecr.outputs.registry }}/epicstory-api
          docker push ${{ steps.login-ecr.outputs.registry }}/epicstory-api
      ```

      My next plan is to include terraform in the workflow to deploy a containerized application in an ec2 instance
      using the published docker image. But for that I am going to need to setup a terraform remote backend on AWS S3 so
      that the terraform state is not persisted in the workflow run and than lost after completion. I need the terraform
      state persisted elsewhere so that I can locally destroy the resources created by the workflow on github actions.

      So far I'm using setup-terraform action

      ```yaml
      - name: Setup terraform
        uses: hashicorp/setup-terraform@v3
      ```

      And including a backend config in main.tf.

      ```terraform
      backend "s3" {
        bucket = "epicstory-terraform-backend"
        key    = "api/terraform.tfstate"
        region = "sa-east-1"
      }
      ```
  - createdAt: 2024-04-22 16:40
    createdBy: omateusmento@gmail.com
    content: |
      Ok, great! I was thankfully able to create the github action. It builds the docker image, publishes it in AWS ECR
      and than run terraform to deploy the containerized application in a ec2 instance.

      The final workflow looks like this:

      ```yaml
      name: Epicstory API CI/CD
      on:
        push:
          branches: infra/api-cicd
      jobs:
        deploy:
          runs-on: ubuntu-latest
          steps:
            - name: Chekout repository
              uses: actions/checkout@v4
            - name: Configure AWS CLI
              uses: aws-actions/configure-aws-credentials@v2
              with:
                aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
                aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                aws-region: ${{ secrets.AWS_REGION }}
            - name: Login Docker to AWS ECR
              id: login-ecr
              uses: aws-actions/amazon-ecr-login@v2
              with:
                registries: ${{ secrets.AWS_REGISTRY }}
            - name: Build and publish docker image
              run: |
                cd api
                docker build -t epicstory-api .
                docker tag epicstory-api ${{ steps.login-ecr.outputs.registry }}/epicstory-api
                docker push ${{ steps.login-ecr.outputs.registry }}/epicstory-api
            - name: Setup terraform
              uses: hashicorp/setup-terraform@v3
            - name: Deploy terraform plan
              run: |
                cd api/deploy
                terraform init
                terraform apply --auto-approve \
                  -var="AWS_ACCESS_KEY_ID=${{secrets.AWS_ACCESS_KEY_ID}}" \
                  -var="AWS_SECRET_ACCESS_KEY=${{secrets.AWS_SECRET_ACCESS_KEY}}" \
                  -var="AWS_REGISTRY=${{secrets.AWS_REGISTRY}}" \
                  -var="AWS_REGION=${{secrets.AWS_REGION}}" \
      ```

      I needed to introduce secrets of aws access keys not only for configuring aws cli in the workflow but for
      injecting them into terraform and configure aws cli inside the deployed ec2 instance so that the docker image
      could be downloaded from AWS ECR and run in the instance. The shell script to deploy the container is as follows:

      ```bash
      # Configure AWS CLI
      echo "Configure AWS CLI"
      aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID}
      aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY}
      aws configure set region ${AWS_REGION}

      # Run application
      echo "Run application"
      aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${AWS_REGISTRY}.dkr.ecr.${AWS_REGION}.amazonaws.com/epicstory-api
      docker run -it -d -p 80:3000 ${AWS_REGISTRY}.dkr.ecr.${AWS_REGION}.amazonaws.com/epicstory-api
      ```

      I need to improve the versioning aspect of the docker image. It always build an image with the "latest" tag and
      publish with that tag, leaving previous latest versions untagged in AWS ECR.

      Also I wish to only run the deployment over new versions.
  - createdAt: 2024-04-22 18:29
    createdBy: omateusmento@gmail.com
    content: |
      To complete this issue I am going to need a strategy to configure updated url of each api and app.
      The api needs the url of app to allow cors origin and the app needs the api url to send request to.
      I still have no idea of how to implement it. I might need to connect to some external service to
      retrieve those urls, or might use a configuration service, or maybe use additional scripts in the
      deployment.
  - createdAt: 2024-04-23 10:56
    createdBy: omateusmento@gmail.com
    content: |
      To solve the url dependency between both api and app, I could either use a external service for
      service discovery or simply run a script to search for the public ips of each deployed instance.
      Other strategy to avoid the dependency is to either deploy the api and app in the same instance
      or run them in the same terraform plan where the public ip is easily retrieve but only for one
      of them because there can't be a cyclic dependency between resources (an additional script would
      be needed to fill the other url dependency).
